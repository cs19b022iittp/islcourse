# -*- coding: utf-8 -*-
"""cs19b022_Exercise 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mqi7Efh28RGq8xBi-GpOmDqG923_t_ug
"""

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor, ToPILImage
from PIL import Image
import torch.nn.functional as F
import numpy as np
from numpy.lib.function_base import average
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def load_data():
  training_data = datasets.FashionMNIST(
      root="data",
      train=True,
      download=True,
      transform=ToTensor(),
  )

  test_data = datasets.FashionMNIST(
      root="data",
      train=False,
      download=True,
      transform=ToTensor(),
  )

  return training_data, test_data

class ModifiedDataset(Dataset):
  def __init__(self,given_dataset,shrink_percent=10):
    self.given_dataset = given_dataset
    self.shrink_percent = shrink_percent
    
  def __len__(self):
    return len(self.given_dataset)

  def __getitem__(self,idx):
    img, lab = self.given_dataset[idx]

    # print (type(img))
    # print (img.shape)

    img2 = transform_tensor_to_pil(img.squeeze())

    # print (img2.size)
    
    new_w = int(img2.size[0]*(1-self.shrink_percent/100.0))
    new_h = int(img2.size[1]*(1-self.shrink_percent/100.0))

    # print (new_w, new_h)

    img3 = img2.resize((new_w,new_h))

    # print (img3.size)

    x = transform_pil_to_tensor(img3)

    # print (x.shape)

    return x,lab

transform_tensor_to_pil = ToPILImage()
transform_pil_to_tensor = ToTensor()

training_data, test_data = load_data()
mod_train_data = ModifiedDataset(training_data)
mod_test_data = ModifiedDataset(test_data)

print (training_data[0][0].shape)
print (mod_train_data[0][0].shape)

def create_dataloaders(training_data,test_data,batch_size=64):
  train_dataloader = DataLoader(training_data,batch_size = batch_size)
  test_dataloader = DataLoader(test_data,batch_size = batch_size)

  for X, y in test_dataloader:
    print(f"Shape of X [N, C, H, W]: {X.shape}")
    print(f"Shape of y: {y.shape} {y.dtype} ")
    break
  
  return train_dataloader, test_dataloader

mod_train_loader, mod_test_loader = create_dataloaders(mod_train_data, mod_test_data, batch_size = 32)

def get_rand_int(ub=10):
  val = int((1+torch.rand(1)*ub).numpy())
  return val

print (get_rand_int(12))

def get_rand_config():
  n_elem = get_rand_int(ub=5)+3
  print (n_elem)

  config = []

  channel_size_list = [1]
  for i in range(n_elem):
    channel_size = get_rand_int(ub=5)
    channel_size_list.append(channel_size)

  for i in range(n_elem):
    in_channels = channel_size_list[i]
    out_channels = channel_size_list[i+1]
    kernel_w = get_rand_int(7)
    kernel_h = get_rand_int(7)
    stride = 1
    padding = 'same'

    elem = (in_channels, out_channels, (kernel_w, kernel_h), stride, padding)

    # print (elem)
    config.append(elem)

  return config

config = get_rand_config()

print (config)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

class cs19b022NN(nn.Module):
  def __init__(self, config, C, H, W):
    super(cs19b022NN, self).__init__()
    self.conv2ds = nn.ModuleList([nn.Conv2d(in_channels, out_channels, kernel, stride, 1) for in_channels, out_channels, kernel, stride, padding in config ])
    self.m = nn.Softmax(dim=1)
    for in_channels, out_channels, kernel, stride, padding in config:
      H = int((H + 2 - kernel[0]) / stride) + 1
      W = int((W + 2 - kernel[1]) / stride) + 1
      C = out_channels
    self.fc = nn.Linear(H*W*C, 10)
    self.relu = nn.ReLU()

  def forward(self, x):
    for conv2d in self.conv2ds:
     x = conv2d(x)
    x = self.relu(x)
    x = torch.flatten(x, 1)
    x = self.fc(x)

    output = self.m(x)
    return output

def loss_fn(y_pred, y_ground):
  v = -(y_ground * torch.log(y_pred + 0.0001))
  v = torch.sum(v)
  return v

# computing loss value on a single data point
for X,y in mod_train_loader:
  model = cs19b022NN(config, X.shape[1], X.shape[2], X.shape[3])
  print(model)
  print(type(X))
  y_pred = model(X)
  y_ground_1h = torch.nn.functional.one_hot(torch.tensor(y), num_classes= 10)
  loss_val = loss_fn(y_pred, y_ground_1h)
  print(loss_val)
  break;

def get_model(training_data_loader, n_epochs):
  for X,y in mod_train_loader:
    N, C, H, W =  X.shape
    break;
  model = cs19b022NN(config, C, H, W).to(device)
  print(model)
  size = len(training_data_loader.dataset)
  optimizer = torch.optim.SGD(model.parameters(), lr=10e-4)
  model.train()
  for epoch in range(1,n_epochs+1):
    print("Epoch ", epoch);
    for batch, (X, y) in enumerate(training_data_loader):
      X, y = X.to(device), y.to(device)

      # Compute prediction error
      pred = model(X)
      y_1h = F.one_hot(y, num_classes= 10)
      loss = loss_fn(pred, y_1h)

      # Backpropagation
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

      if batch % 100 == 0:
          loss, current = loss.item(), batch * len(X)
          print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")
  return model



def test(test_data_loader, model):
  size = len(test_data_loader.dataset)
  num_batches = len(test_data_loader)
  model.eval()

  actual = []   
  predicted = [] 
  test_loss, correct = 0, 0
  with torch.no_grad():
      for X, y in test_data_loader:
           X, y = X.to(device), y.to(device)
           y1 = model(X)
           actual.append(y)
           predicted.append(y1.argmax(1))
           y_h = F.one_hot(y, num_classes= 10)
           test_loss += loss_fn(y1, y_h).item()
           correct += (y1.argmax(1) == y).type(torch.float).sum().item()
           
        
  test_loss /= num_batches
  correct /= size
  print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")

  predicted = [val.item() for sublist in predicted for val in sublist]
  actual = [val.item() for sublist in actual for val in sublist]


  accuracy_val = correct
  precision_val = precision_score(actual, predicted, average='macro')
  recall_val = recall_score(actual, predicted, average='macro')
  f1_val= f1_score(actual, predicted, average='macro')

  print(f"Accuracy : {(accuracy_val):>0.4f}"),
  print(f"Precision: {(precision_val):>0.4f}")
  print(f"Recall   : {(recall_val):>0.4f}")
  print(f"F1 scores: {(f1_val):>0.4f}")
